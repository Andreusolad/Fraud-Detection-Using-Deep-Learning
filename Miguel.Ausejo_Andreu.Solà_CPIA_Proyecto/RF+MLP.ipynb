{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "# DATASET LINK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Link al dataset de Kaggle usado en el proyecto:\n",
        "\"https://www.kaggle.com/datasets/tusharbhadouria/credit-card-fraud-detection\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# INSTALL PANDAS AND NUMPY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Importamos las librerías necesarias para el análisis de datos, manipulación y visualización: pandas y numpy.\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LOAD DATASETS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Cargamos los datos de entrenamiento y test desde archivos CSV. Como los datasets son muy grandes y tardan en cargarse, utilizamos una librería para paralelizar el código de carga.\n",
        "# Librería utilizada: multiprocessing\n",
        "from multiprocessing.pool import ThreadPool\n",
        "\n",
        "# Función para cargar los archivos csv que tenemos como datasets.\n",
        "def carregar(nom):\n",
        "    return pd.read_csv(nom)\n",
        "\n",
        "# Los csv que cargaremos son los siguientes:\n",
        "csvs = ['fraudTrain.csv', 'fraudTest.csv']\n",
        "\n",
        "# Creamos 2 grupos porque tenemoso dos archivos csv.\n",
        "pool = ThreadPool(2)\n",
        "\n",
        "# Cargamos los archivos en paralelo.\n",
        "resultats = pool.map(carregar, csvs)\n",
        "\n",
        "# Guardamos los resultados en sus respectivas variables de dataset de entrenamiento y test.\n",
        "train_data = resultats[0]\n",
        "test_data = resultats[1]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Visualizamos las primeras filas del conjunto de datos de entrenamiento para entender su estructura.\n",
        "train_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "trusted": true
      },
      "source": [
        "## DATA PREPROCESSING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mostramos las dimensiones de los conjuntos de datos de entrenamiento y test, para verificar la cantidad de muestras y características.\n",
        "print(f'{train_data.shape}')\n",
        "print(f'{test_data.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Obtenemos información detallada sobre el conjunto de datos de entrenamiento, incluyendo tipos de datos y valores nulos, para evaluar la calidad de los datos y saber si necesitamos realizar limpieza o preprocesamiento adicional..\n",
        "train_data.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### NO NULL VALUES\n",
        "Podemos ver que no hay valores nulos en ninguna columna. No hay nulls en todo el dataset, por lo tanto, todos los datos son válidos en este aspecto y no nos tenemos que preocupar por ello. No obstante, ahora tenemos que procesar los datos: transformar objetos y strings a valors numéricos. Además, columnas de IDs como cc_num y trans_num se pueden eliminar del dataset porque los valores de identidad no nos van a aportar ningún tipo de información de valor en este proyecto.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### TRANSFORM DATES TO TIMESTAMP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Primero, eliminamos la primera columna del dataset, que es un índice y no aporta ningún tipo de información relevante para el análisis.\n",
        "train_data = train_data.drop(columns='Unnamed: 0')\n",
        "test_data = test_data.drop(columns='Unnamed: 0')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ahora, convertimos las fechas a números, específicamente a timestamps en segundos desde la época Unix (1 de enero de 1970).\n",
        "train_data['trans_date_trans_time'] = pd.to_datetime(train_data['trans_date_trans_time']).astype('int64') // 10**9\n",
        "test_data['trans_date_trans_time'] = pd.to_datetime(test_data['trans_date_trans_time']).astype('int64') // 10**9\n",
        "\n",
        "# Convertimos la columna 'dob' (fecha de nacimiento) a timestamps en segundos.\n",
        "train_data['dob'] = pd.to_datetime(train_data['dob']).astype('int64') // 10**9\n",
        "test_data['dob'] = pd.to_datetime(test_data['dob']).astype('int64') // 10**9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualizamos las primeras filas del conjunto de datos de entrenamiento para verificar los cambios realizados.\n",
        "train_data.head(1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualizamos las primeras filas del conjunto de datos de test para verificar los cambios realizados.\n",
        "test_data.head(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### AÚN FALTA TRANSFORMAR ALGUNAS COLUMNAS A VALORES NÚMEROS, Y POR ELLO UTILIZAMOS UN ENCODER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Aquí, utilizamos OrdinalEncoder de sklearn para convertir las columnas categóricas en números, facilitando así su uso en modelos de machine learning.\n",
        "# Para ello, importamos sklearn.preprocessing.OrdinalEncoder.\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "\n",
        "# Eliminamos las columnas de IDs que no aportan valor predictivo.\n",
        "train_data = train_data.drop(columns=['cc_num', 'trans_num'])\n",
        "test_data = test_data.drop(columns=['cc_num', 'trans_num'])\n",
        "\n",
        "# Lista de las columnas a codificar con el OrdinalEncoder.\n",
        "categorical_cols = ['gender', 'category', 'merchant', 'job', 'city', 'state', 'street', 'first', 'last']\n",
        "\n",
        "# Creamos un OrdinalEncoder que se encargue de valores desconocidos.\n",
        "encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1, dtype=int)\n",
        "\n",
        "# Entrenamos el Encoder con los datos de entrenamiento (paso previo necesario para la paralelización).\n",
        "encoder.fit(train_data[categorical_cols])\n",
        "\n",
        "# Función para aplicar la transformación en paralelo.\n",
        "def aplicar_encoder(df):\n",
        "    # Entrenamos el Encoder en los datos de entrenamiento y los transformamos (asigna el número por orden alfabético):\n",
        "    # Transformamos los datos del test usando el encoder entrenado, y los valores desconocidos que no ha visto en el entrenamiento se codifican con un -1.\n",
        "    df[categorical_cols] = encoder.transform(df[categorical_cols])\n",
        "    return df\n",
        "\n",
        "# Creamos un grupo de 2 hilos para procesar train y test simultáneamente.\n",
        "pool_enc = ThreadPool(2)\n",
        "resultats_list = pool_enc.map(aplicar_encoder, [train_data, test_data])\n",
        "\n",
        "# Recuperamos los resultados procesados.\n",
        "train_data, test_data = resultats_list[0], resultats_list[1]\n",
        "\n",
        "# Visualizamos los resultados codificados\n",
        "train_data.head(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TRAIN SIMPLE MODELS TO HAVE AN IDEA OF % OF ACCURACY, RECALL AND F1-SCORE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Separamos las características (X_) de la variable objetivo (y_) del dataset de entrenamiento.\n",
        "X_train = train_data.drop(columns='is_fraud')\n",
        "y_train = train_data['is_fraud']\n",
        "\n",
        "# Hacemos lo mismo para el dataset de test.\n",
        "X_test = test_data.drop(columns='is_fraud')\n",
        "y_test = test_data['is_fraud']  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Importamos RandomForestClassifier de sklearn para entrenar un modelo de clasificación.\n",
        "import sklearn\n",
        "from sklearn.ensemble import RandomForestClassifier\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Entrenamos un modelo de Random Forest con los datos de entrenamiento y los parámetros especificados.\n",
        "# Importamos os para poder contar el número de cpus que tenemos, y vamos a usar un porcentaje de ellas, un 75%, que trabajen de forma paralela cuando entrenamos el modelo del Random Forest.\n",
        "# Paralelizando así el código se ejecuta más rápido, pero dejamos un 25% de cpus disponibles para otras tareas del ordenador y que este no se cuelgue y pare.\n",
        "import os\n",
        "n_cores_mlp = os.cpu_count()\n",
        "safe_cores_rf = max(1, int(n_cores_mlp * 0.75))\n",
        "model = RandomForestClassifier(n_estimators=10, max_depth=10, random_state=2003, n_jobs=safe_cores_rf)\n",
        "model.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hacemos las predicciones en el conjunto de test.\n",
        "y_pred = model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Importamos las métricas necesarias para evaluar el rendimiento del modelo, concretamente accuracy, recall y f1-score.\n",
        "from sklearn.metrics import accuracy_score, recall_score\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1score = sklearn.metrics.f1_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy}')\n",
        "print(f'Recall: {recall}')\n",
        "print(f'F1-Score: {f1score}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Importamos Optuna y los módulos necesarios para la optimización de hiperparámetros. Aquí obtendremos los mejores parámetros para maximizar el recall (detección de fraudes) del Random Forest.\n",
        "import optuna\n",
        "from optuna.pruners import MedianPruner # MedianPruner para detener ensayos que no muestran mejora.\n",
        "from optuna.samplers import TPESampler # TPE Sampler para una búsqueda eficiente de hiperparámetros. Lo que hace es una optimización bayesiana para seleccionar los mejores hiperparámetros basándose en los resultados de ensayos anteriores.\n",
        "\n",
        "# Definimos la función objetivo que Optuna utilizará para evaluar diferentes combinaciones de hiperparámetros. En ella, entrenamos un RandomForestClassifier con los parámetros de prueba y calculamos el recall en el conjunto de test.\n",
        "# Luego, Optuna intentará maximizar este valor, buscando los mejores hiperparámetros y mejorando así la capacidad del modelo para detectar fraudes.\n",
        "\n",
        "def objective(trial):\n",
        "    # Definimos los hiperparámetros a optimizar.\n",
        "    n_estimators = trial.suggest_int('n_estimators', 20, 50) # Número de árboles en el bosque.\n",
        "    \n",
        "    # Ahora, debemos ver si max_depth es None o un valor específico.\n",
        "    max_depth_choice = trial.suggest_int('max_depth_choice', 0, 1) # Si es 0, max_depth será un valor entre 10 y 50; si es 1, será None. Esto se deber hacer así porque Optuna no permite sugerir None directamente.\n",
        "    max_depth = None if max_depth_choice == 1 else trial.suggest_int('max_depth', 10, 50) \n",
        "    \n",
        "    min_samples_split = trial.suggest_int('min_samples_split', 2, 10) # Mínimo número de muestras necesarias para dividir un nodo.\n",
        "    \n",
        "    min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 5) # Mínimo número de muestras necesarias en una hoja.\n",
        "    \n",
        "    max_features = trial.suggest_categorical('max_features', ['sqrt', 'log2', None]) # Número de características a considerar al buscar la mejor división. \n",
        "    # 'sqrt' usa la raíz cuadrada del número total de características, 'log2' usa el logaritmo base 2 y None usa todas las características. \n",
        "    \n",
        "    \n",
        "    \n",
        "    # Creamos el Random Forest con los hiperparámetros sugeridos.\n",
        "    rf = RandomForestClassifier(\n",
        "        n_estimators=n_estimators,\n",
        "        max_depth=max_depth,\n",
        "        min_samples_split=min_samples_split,\n",
        "        min_samples_leaf=min_samples_leaf,\n",
        "        max_features=max_features,\n",
        "        random_state=2003\n",
        "    )\n",
        "    \n",
        "    # Entrenamos el modelo con los datos de entrenamiento.\n",
        "    rf.fit(X_train, y_train)\n",
        "    \n",
        "    # Hacemos las predicciones en el conjunto de test.\n",
        "    y_pred = rf.predict(X_test)\n",
        "    \n",
        "    # Calculamos el recall (nuestro objetivo de optimización).\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    \n",
        "    return recall\n",
        "\n",
        "# Creamos un estudio de Optuna para maximizar el recall, utilizando el TPE Sampler para una búsqueda eficiente y el Median Pruner para detener ensayos que no muestran mejora.\n",
        "# Luego usaremos este estudio para optimizar los hiperparámetros del Random Forest.\n",
        "study = optuna.create_study(\n",
        "    direction='maximize', # direction='maximize' porque queremos maximizar el recall.\n",
        "    sampler=TPESampler(seed=2003), # Random State fijado para reproducibilidad.\n",
        "    pruner=MedianPruner() # Usamos MedianPruner para detener ensayos que no muestran mejora.\n",
        ")\n",
        "\n",
        "# Ahora, optimizamos los hiperparámetros ejecutando múltiples ensayos. \n",
        "# Aquí lo que hacemos es ejecutar la función objetivo 50 veces, cada vez con una combinación diferente de hiperparámetros sugerida por Optuna, para encontrar la que maximiza el recall.\n",
        "study.optimize(objective, n_trials=50, show_progress_bar=True, n_jobs=safe_cores_rf)\n",
        "\n",
        "# Guardamos los mejores resultados obtenidos tras la optimización de hiperparámetros.\n",
        "best_trial = study.best_trial\n",
        "print(f\"\\nBest trial:\")\n",
        "print(f\"  Recall: {best_trial.value:.4f}\")\n",
        "print(f\"  Parameters: {best_trial.params}\")\n",
        "\n",
        "# Entrenamos el modelo final con los mejores parámetros encontrados por Optuna.\n",
        "best_params = best_trial.params\n",
        "# Ajustamos max_depth según la elección hecha durante la optimización.\n",
        "max_depth_final = None if best_params['max_depth_choice'] == 1 else best_params.get('max_depth')\n",
        "\n",
        "# Finalmente, definimos el mejor modelo con los mejores hiperparámetros encontrados.\n",
        "best_model = RandomForestClassifier(n_estimators=best_params['n_estimators'], max_depth=max_depth_final, min_samples_split=best_params['min_samples_split'],\n",
        "    min_samples_leaf=best_params['min_samples_leaf'], max_features=best_params['max_features'], random_state=2003)\n",
        "\n",
        "# Entrenamos el mejor modelo con los datos de entrenamiento.\n",
        "best_model.fit(X_train, y_train)\n",
        "\n",
        "# Hacemos las predicciones en el conjunto de test con el mejor modelo.\n",
        "y_pred_best = best_model.predict(X_test)\n",
        "\n",
        "# Por último, calculamos y mostramos las métricas de rendimiento del mejor modelo en el conjunto de test.\n",
        "accuracy_best = accuracy_score(y_test, y_pred_best)\n",
        "recall_best = recall_score(y_test, y_pred_best)\n",
        "f1_best = sklearn.metrics.f1_score(y_test, y_pred_best)\n",
        "\n",
        "print(f\"\\nBest Model Performance on Test Set:\")\n",
        "print(f'Accuracy: {accuracy_best:.4f}')\n",
        "print(f'Recall: {recall_best:.4f}')\n",
        "print(f'F1-Score: {f1_best:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## MLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# En esta sección vamos a optimizar los hiperparámetros de un MLPClassifier. Para ello, importamos las librerías siguientes, brevemente explicadas:\n",
        "# - optuna: Librería para optimización de hiperparámetros.\n",
        "# - sklearn.metrics: Módulo para evaluar el rendimiento del modelo.\n",
        "# - sklearn.model_selection: Módulo para validación cruzada.\n",
        "# - sklearn.neural_network: Módulo que contiene el clasificador MLP.\n",
        "# - sklearn.preprocessing: Módulo para escalado de características.\n",
        "# - imblearn.over_sampling: Módulo para aplicar SMOTE.\n",
        "# - imblearn.pipeline: Módulo para crear pipelines que incluyen SMOTE.\n",
        "# - import os, psutil: Módulos para gestionar recursos del sistema, ya que esta optimización a veces puede consumir muchos recursos.\n",
        "import optuna\n",
        "from optuna.pruners import MedianPruner\n",
        "from optuna.samplers import TPESampler\n",
        "from sklearn.metrics import accuracy_score, recall_score, f1_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.preprocessing import StandardScaler  \n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.pipeline import Pipeline\n",
        "import os\n",
        "import psutil\n",
        "\n",
        "# Necesitamos realizar este paso porque el entrenamiento de MLP puede ser intensivo en recursos, y esto hace que nuestro ordenador se cuelgue si no lo controlamos bien.\n",
        "# En este paso, miramos cuantos cpus tenemos disponibles y vamos a usar un porcentaje de ellos, no todos porque podemos hacer que nuestro \n",
        "# ordenador se cuelgue\n",
        "n_cores_mlp = os.cpu_count()\n",
        "\n",
        "print(f\"Available CPU cores: {n_cores_mlp}\")\n",
        "\n",
        "# Aquí determinamos que vamos a usar el 75% de los cores disponibles en nuestro ordenador.\n",
        "safe_cores_mlp = max(1, int(n_cores_mlp * 0.75))\n",
        "\n",
        "\n",
        "# Definimos la función de optimización con Optuna para el MLP. Evaluaremos diferentes combinaciones de hiperparámetros y entrenaremos distintos MLP con esos parámetros. Entonces,\n",
        "# Optuna intentará buscar los mejores hiperparámetros, tal y como hemos hecho en el Random Forest anteriormente. \n",
        "\n",
        "def objective_mlp(trial, X, y):\n",
        "    # Definimos los hiperparámetros a optimizar para el MLP.\n",
        "    hidden_layer_sizes = trial.suggest_categorical('hidden_layer_sizes', [(50,), (100,), (50, 50), (100, 50)]) # Provamos distintas arquitecturas de capas del MLP.\n",
        "    activation = trial.suggest_categorical('activation', ['relu', 'tanh']) # Provamos distintas activaciones del MLP.\n",
        "    solver = trial.suggest_categorical('solver', ['adam']) # Como solver usamos adam porque sgd a veces es demasiado lento.\n",
        "    alpha = trial.suggest_float('alpha', 1e-5, 1e-2, log=True) # El parámetro alpha, que ayuda a prevenir el overfitting si el modelo es complejo.\n",
        "    learning_rate_init = trial.suggest_float('learning_rate_init', 1e-4, 1e-2, log=True) # El learning rate controla la velocidad del ajuste de pesos.\n",
        "    batch_size = trial.suggest_categorical('batch_size', [64, 128, 256]) # El batch size es el número de muestras que procesa el modelo antes de actualizar sus pesos.\n",
        "    \n",
        "    # Creamos el MLP con los valores que acabos de definir anteriormente, para más adelante encontrar los valores más óptimos.\n",
        "    mlp = MLPClassifier(\n",
        "        hidden_layer_sizes=hidden_layer_sizes,\n",
        "        activation=activation,\n",
        "        solver=solver,\n",
        "        alpha=alpha,\n",
        "        learning_rate_init=learning_rate_init,\n",
        "        batch_size=batch_size,\n",
        "        max_iter=200, # Lo mantenemos bajo para la velocidad. \n",
        "        random_state=2003,\n",
        "        early_stopping=True, # Usamos el early stopping para más velocidad y no perder tiempo si el modelo se estanca. \n",
        "        validation_fraction=0.1, # Fracción de los datos de entrenamiento que se utiliza para validar el modelo. \n",
        "        n_iter_no_change=10, # Iteraciones que puede hacer el modelo antes de detenerse si no hay mejora.\n",
        "        verbose=0\n",
        "    )\n",
        "    \n",
        "    # Instanciamos SMOTE porque nuestro dataset es muy desequilibrado, y con SMOTE creamos datos \"sintéticos\" para equilibrarlo. Es decir, hacemos oversampling de la clase minoritaria.\n",
        "    smote = SMOTE(random_state=2003, k_neighbors=3)\n",
        "    \n",
        "    # Instanciamos el StandardScaler para escalar nuestros datos. \n",
        "    scaler = StandardScaler()\n",
        "    \n",
        "    # Creamos la Pipeline para seguir el orden correcto de escalar, aplicar el smote y entrenar el MLP.\n",
        "    pipeline = Pipeline([\n",
        "        ('scaler', scaler), \n",
        "        ('smote', smote),\n",
        "        ('mlp', mlp)\n",
        "    ])\n",
        "    \n",
        "    # Usando el número de cores especificados, calculamos la métrica del recall con cross_val_score. \n",
        "    cv_cores = max(1, safe_cores_mlp // 2)\n",
        "    scores = cross_val_score(pipeline, X, y, scoring='recall', cv=3, n_jobs=cv_cores)\n",
        "    recall_mean = scores.mean()\n",
        "    \n",
        "\n",
        "    \n",
        "    return recall_mean\n",
        "\n",
        "# Creamos un objeto de estudio para poder maximizar el recall, tal y como hemos hecho antes en el Random Forest, con parámetros parecidos. \n",
        "study_mlp = optuna.create_study(\n",
        "    direction='maximize', # direction='maximize' porque queremos maximizar el recall.\n",
        "    sampler=TPESampler(seed=2003), # Para reproducibilidad\n",
        "    pruner=MedianPruner() # Usamos MedianPruner para detener ensayos que no muestran mejora.\n",
        ")\n",
        "\n",
        "\n",
        "# Para el estudio, usamos un total de cores que sea seguro para nuestro sistema, tal y como hemos hecho antes.\n",
        "parallel_trials_mlp = max(1, safe_cores_mlp // 2)\n",
        "\n",
        "# Ejecutamos la optimización con los parámetros que hemos definido ya.\n",
        "study_mlp.optimize(\n",
        "    lambda trial: objective_mlp(trial, X_train, y_train),\n",
        "    n_trials=20,\n",
        "    show_progress_bar=True,\n",
        "    n_jobs=parallel_trials_mlp\n",
        ")\n",
        "\n",
        "# Cogemos el mejor estudio que ha obtenido la mejor solución. Con los parámetros ya especificados, vemos cuál es el mejor recall obtenido. \n",
        "best_trial_mlp = study_mlp.best_trial\n",
        "print(f\"\\n\\nBest MLP trial from cross-validation:\")\n",
        "print(f\"  Mean CV Recall: {best_trial_mlp.value:.4f}\") # Mejor recall medio.\n",
        "print(f\"  Parameters: {best_trial_mlp.params}\") # Mejores parámetros.\n",
        "\n",
        "\n",
        "# Ahora, entrenamos el MLP con los mejores parámetros que hemos obtenido.\n",
        "\n",
        "# Primero de todo, escalamos los datos con el StandardScaler tanto los de entrenamiento como de test. \n",
        "scaler_final = StandardScaler()\n",
        "X_train_scaled = scaler_final.fit_transform(X_train)\n",
        "X_test_scaled = scaler_final.transform(X_test) \n",
        "\n",
        "# Luego, aplicamos el SMOTE porque el dataset está bastante desbalanceado. De esta manera, tenemos más fraudes para que el modelo los pueda analizar. \n",
        "smote_final_mlp = SMOTE(random_state=2003, k_neighbors=3)\n",
        "X_train_smote_mlp, y_train_smote_mlp = smote_final_mlp.fit_resample(X_train_scaled, y_train) # Dataset con SMOTE aplicado ya.\n",
        "\n",
        "\n",
        "# Evidentemente, el MLP que vamos a entrenar va a tener los mejores hiperparámetros que ya hemos encontrado antes.\n",
        "best_params_mlp = best_trial_mlp.params\n",
        "\n",
        "# Creamos el MLP con los parámetros.\n",
        "best_mlp_model = MLPClassifier(\n",
        "    hidden_layer_sizes=best_params_mlp['hidden_layer_sizes'],\n",
        "    activation=best_params_mlp['activation'],\n",
        "    solver=best_params_mlp['solver'],\n",
        "    alpha=best_params_mlp['alpha'],\n",
        "    learning_rate_init=best_params_mlp['learning_rate_init'],\n",
        "    batch_size=best_params_mlp['batch_size'],\n",
        "    max_iter=500,\n",
        "    random_state=2003,\n",
        "    early_stopping=True,\n",
        "    validation_fraction=0.1,\n",
        "    n_iter_no_change=10,\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "# Entrenamos el modelo con el dataset con SMOTE aplicado.\n",
        "best_mlp_model.fit(X_train_smote_mlp, y_train_smote_mlp)\n",
        "\n",
        "# Hacemos las predicciones una vez entrenado el modelo sobre el dataset de X_test_scaled, este sin SMOTE porque es el de test.\n",
        "y_pred_mlp = best_mlp_model.predict(X_test_scaled)\n",
        "\n",
        "# A partir de las predicciones, ahora podemos calcular las distintas métricas que nos interesan y que ya hemos visto antes.\n",
        "accuracy_mlp = accuracy_score(y_test, y_pred_mlp)\n",
        "recall_mlp = recall_score(y_test, y_pred_mlp)\n",
        "f1_mlp = f1_score(y_test, y_pred_mlp)\n",
        "\n",
        "print(f\"\\nFinal MLP Model Performance on Test Set:\")\n",
        "print(f'  Accuracy:  {accuracy_mlp:.4f}')\n",
        "print(f'  Recall:    {recall_mlp:.4f}')\n",
        "print(f'  F1-Score:  {f1_mlp:.4f}')\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
