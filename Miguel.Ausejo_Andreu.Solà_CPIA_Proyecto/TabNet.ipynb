{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d16b5308",
   "metadata": {},
   "source": [
    "## SECCIÓN TABNET\n",
    "\n",
    "#### PARTE 1: IMPORT DE LIBRERÍAS Y DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e61b1c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "328649a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos los datos de entrenamiento y test desde archivos CSV. Como los datasets son muy grandes y tardan en cargarse, utilizamos una librería para paralelizar el código de carga.\n",
    "# Librería utilizada: multiprocessing\n",
    "from multiprocessing.pool import ThreadPool\n",
    "\n",
    "# Función para cargar los archivos csv que tenemos como datasets.\n",
    "def carregar(nom):\n",
    "    return pd.read_csv(nom)\n",
    "\n",
    "# Los csv que cargaremos son los siguientes:\n",
    "csvs = ['fraudTrain.csv', 'fraudTest.csv']\n",
    "\n",
    "# Creamos 2 grupos porque tenemoso dos archivos csv.\n",
    "pool = ThreadPool(2)\n",
    "\n",
    "# Cargamos los archivos en paralelo.\n",
    "resultats = pool.map(carregar, csvs)\n",
    "\n",
    "# Guardamos los resultados en sus respectivas variables de dataset de entrenamiento y test.\n",
    "train_data = resultats[0]\n",
    "test_data = resultats[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8f0630",
   "metadata": {},
   "source": [
    "#### PARTE 2: DATA-PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7191c0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# En esta parte eliminamos la columna 'Unnamed: 0' que no aporta información relevante ya que es un índice\n",
    "train_data = train_data.drop(columns='Unnamed: 0')\n",
    "test_data = test_data.drop(columns='Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba891352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiempo Secuencial: 1.0269 segundos\n"
     ]
    }
   ],
   "source": [
    "import time # Importamos la librería time para medir el tiempo de ejecución\n",
    "\n",
    "# Convertimos la columna 'trans_date_trans_time' a formato timestamp para los dos conjuntos de datos y la columna 'dob' (date of birth) a formato timestamp para los dos conjuntos de datos\n",
    "# En esta celda lo hacemos sin implementar paralelización para comparar el rendimiento con el mismo proceso pero con paralelización que hacemos en la siguiente celda\n",
    "start_time_seq = time.time()\n",
    "\n",
    "train_data['trans_date_trans_time'] = pd.to_datetime(train_data['trans_date_trans_time']).astype('int64') // 10**9 # Aquí convertimos la columna a timestamp en segundos mediante pandas, donde astype('int64') convierte la fecha a nanosegundos desde epoch y dividimos por 10**9 para obtener segundos\n",
    "test_data['trans_date_trans_time'] = pd.to_datetime(test_data['trans_date_trans_time']).astype('int64') // 10**9 # Hacemos lo mismo para el conjunto de test\n",
    "\n",
    "train_data['dob'] = pd.to_datetime(train_data['dob']).astype('int64') // 10**9 # Convertimos la columna 'dob' a timestamp en segundos\n",
    "test_data['dob'] = pd.to_datetime(test_data['dob']).astype('int64') // 10**9 # Hacemos lo mismo para el conjunto de test\n",
    "\n",
    "end_time_seq = time.time()\n",
    "tiempo_seq = end_time_seq - start_time_seq\n",
    "print(f\"Tiempo Secuencial: {tiempo_seq:.4f} segundos\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a10f2614",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Hacemos lo mismo que en la celda anterior pero con paralelización para mejorar la velocidad\\nfrom multiprocessing import Pool, cpu_count\\n\\n# Definimos una primera función que procese un chunk del DataFrame\\ndef process_dates(df_chunk):\\n    if \\'trans_date_trans_time\\' in df_chunk.columns:\\n        df_chunk[\\'trans_date_trans_time\\'] = pd.to_datetime(df_chunk[\\'trans_date_trans_time\\']).astype(\\'int64\\') // 10**9 # Convertimos a timestamp en segundos de la fecha y hora de la transacción\\n\\n    if \\'dob\\' in df_chunk.columns:\\n        df_chunk[\\'dob\\'] = pd.to_datetime(df_chunk[\\'dob\\']).astype(\\'int64\\') // 10**9 # Convertimos a timestamp en segundos de fecha de nacimiento\\n\\n    return df_chunk\\n\\n# Definimos una función para paralelizar el procesamiento del DataFrame\\ndef parallelize_dataframe(df, func, n_cores=None):\\n    if n_cores is None: # Usamos todos los núcleos disponibles menos uno para no saturar el sistema\\n        n_cores = cpu_count() - 1 \\n\\n    df_split = np.array_split(df, n_cores) # Dividimos el DataFrame en n_cores partes\\n\\n    with Pool(n_cores) as pool: # Creamos un pool de procesos\\n        df = pd.concat(pool.map(func, df_split)) # Aplicamos la función a cada parte en paralelo y juntamos los resultados\\n\\n    return df\\n\\nstart_time_par = time.time()\\n\\ntrain_data = parallelize_dataframe(train_data, process_dates)\\ntest_data = parallelize_dataframe(test_data, process_dates)\\n\\nend_time_par = time.time()\\ntiempo_par = end_time_par - start_time_par\\nprint(f\"Tiempo Paralelo: {tiempo_par:.4f} segundos\")\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Hacemos lo mismo que en la celda anterior pero con paralelización para mejorar la velocidad\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "# Definimos una primera función que procese un chunk del DataFrame\n",
    "def process_dates(df_chunk):\n",
    "    if 'trans_date_trans_time' in df_chunk.columns:\n",
    "        df_chunk['trans_date_trans_time'] = pd.to_datetime(df_chunk['trans_date_trans_time']).astype('int64') // 10**9 # Convertimos a timestamp en segundos de la fecha y hora de la transacción\n",
    "    \n",
    "    if 'dob' in df_chunk.columns:\n",
    "        df_chunk['dob'] = pd.to_datetime(df_chunk['dob']).astype('int64') // 10**9 # Convertimos a timestamp en segundos de fecha de nacimiento\n",
    "        \n",
    "    return df_chunk\n",
    "\n",
    "# Definimos una función para paralelizar el procesamiento del DataFrame\n",
    "def parallelize_dataframe(df, func, n_cores=None):\n",
    "    if n_cores is None: # Usamos todos los núcleos disponibles menos uno para no saturar el sistema\n",
    "        n_cores = cpu_count() - 1 \n",
    "    \n",
    "    df_split = np.array_split(df, n_cores) # Dividimos el DataFrame en n_cores partes\n",
    "    \n",
    "    with Pool(n_cores) as pool: # Creamos un pool de procesos\n",
    "        df = pd.concat(pool.map(func, df_split)) # Aplicamos la función a cada parte en paralelo y juntamos los resultados\n",
    "        \n",
    "    return df\n",
    "\n",
    "start_time_par = time.time()\n",
    "\n",
    "train_data = parallelize_dataframe(train_data, process_dates)\n",
    "test_data = parallelize_dataframe(test_data, process_dates)\n",
    "\n",
    "end_time_par = time.time()\n",
    "tiempo_par = end_time_par - start_time_par\n",
    "print(f\"Tiempo Paralelo: {tiempo_par:.4f} segundos\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6abc3380",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Comprobamos la mejora de tiempo con paralelización\\nspeedup = tiempo_seq / tiempo_par\\nprint(f\"El speedup es de: {speedup:.2f}x\")\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Comprobamos la mejora de tiempo con paralelización\n",
    "speedup = tiempo_seq / tiempo_par\n",
    "print(f\"El speedup es de: {speedup:.2f}x\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd650b91",
   "metadata": {},
   "source": [
    "#### PARTE 3: PREPARAMOS LOS DATOS Y INICIAMOS EL ENCODING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1ebc36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos una función que agrupa las tareas independientes para cada dataset.\n",
    "def preprocesar_features(df):\n",
    "    # En esta parte eliminamos las columnas que no aportan información relevante para este modelo\n",
    "    df = df.drop(columns=['first', 'last', 'street', 'city', 'zip', 'job', 'merchant','merch_lat','merch_long','trans_num'])\n",
    "\n",
    "    # Aquí aplicamos el one-hot encoding a las columnas categóricas, creando variables binarias para cada categoría, incrementando así la cantidad de columnas en ambos conjuntos de datos\n",
    "    df = pd.get_dummies(df, columns=['gender', 'category', 'state'], drop_first=True)\n",
    "    return df\n",
    "\n",
    "# Creamos un grupo de 2 hilos para procesar las transformaciones de train y test simultáneamente.\n",
    "pool_dummies = ThreadPool(2)\n",
    "resultats_dummies = pool_dummies.map(preprocesar_features, [train_data, test_data])\n",
    "\n",
    "# Recuperamos los datos procesados.\n",
    "train_data, test_data = resultats_dummies[0], resultats_dummies[1]\n",
    "\n",
    "# En esta parte alineamos las columnas del conjunto de test con las del conjunto de entrenamiento, por tal de tener el mismo númerod de columnas en ambos conjuntos de datos\n",
    "# Pueden tener tamaños diferentes debido al one-hot encoding, ya que algunas categorías pueden estar presentes en el conjunto de training al ser más grande, pero no en el conjunto de test\n",
    "train_cols = train_data.columns\n",
    "test_data = test_data.reindex(columns=train_cols, fill_value=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7484041e",
   "metadata": {},
   "source": [
    "#### PARTE 4: PROGRAMAMOS EL ALGORITMO DE TABNET (red neuronal + árbol de decisión)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2dd5374",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\andre\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.18085 | val_auc: 0.96747 |  0:02:10s\n",
      "epoch 1  | loss: 0.10812 | val_auc: 0.99137 |  0:04:26s\n",
      "epoch 2  | loss: 0.0945  | val_auc: 0.99311 |  0:06:57s\n",
      "epoch 3  | loss: 0.08679 | val_auc: 0.98711 |  0:09:09s\n",
      "epoch 4  | loss: 0.08013 | val_auc: 0.99099 |  0:11:20s\n",
      "epoch 5  | loss: 0.07303 | val_auc: 0.98948 |  0:13:29s\n",
      "epoch 6  | loss: 0.0693  | val_auc: 0.98704 |  0:15:35s\n",
      "epoch 7  | loss: 0.06591 | val_auc: 0.98608 |  0:17:41s\n",
      "epoch 8  | loss: 0.06284 | val_auc: 0.98483 |  0:19:46s\n",
      "epoch 9  | loss: 0.06103 | val_auc: 0.98302 |  0:21:50s\n",
      "epoch 10 | loss: 0.05687 | val_auc: 0.97804 |  0:24:03s\n",
      "epoch 11 | loss: 0.05593 | val_auc: 0.98592 |  0:26:14s\n",
      "epoch 12 | loss: 0.0525  | val_auc: 0.97754 |  0:28:25s\n",
      "\n",
      "Early stopping occurred at epoch 12 with best_epoch = 2 and best_val_auc = 0.99311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\andre\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    }
   ],
   "source": [
    "# Importamos las librerías necesarias para el modelo TabNet\n",
    "import torch\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "# Sepramos las características (X) de la variable objetivo (y) de los dos subgrupos\n",
    "X_full = train_data.drop(columns='is_fraud').values\n",
    "y_full = train_data['is_fraud'].values\n",
    "\n",
    "X_test_final = test_data.drop(columns='is_fraud').values\n",
    "y_test_final = test_data['is_fraud'].values\n",
    "\n",
    "# Dividimos el set de datos para entrenamiento en training y validación, usando stratify para mantener la proporción de clases\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_full, y_full, test_size=0.3, random_state=2003, stratify=y_full)\n",
    "\n",
    "# Escalamos los datos usando RobustScaler\n",
    "scaler = RobustScaler()\n",
    "# Hacemos el fit y transformamos los datos de entrenamiento\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "# Transformamos los datos de validación y test final usando el mismo scaler\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test_final = scaler.transform(X_test_final)\n",
    "\n",
    "\n",
    "# Creamos el modelo TabNet\n",
    "clf = TabNetClassifier(\n",
    "    optimizer_fn=torch.optim.Adam,  # escogemos Adam como optimizador por su buen rendimiento\n",
    "    optimizer_params=dict(lr=2e-2), # escogemos una tasa de aprendizaje inicial de 0.02\n",
    "    scheduler_params={\"step_size\":10, \"gamma\":0.9}, # escogemos un scheduler que reduce la tasa de aprendizaje cada 10 epochs multiplicándola por 0.9\n",
    "    scheduler_fn=torch.optim.lr_scheduler.StepLR, # escogemos StepLR como scheduler, lo cual sirve para reducir la tasa de aprendizaje durante el entrenamiento\n",
    "    mask_type='entmax' # por ultimo, escogemos 'entmax' como tipo de máscara para el modelo TabNet, lo cual se utiliza para la atención en las características importantes\n",
    ")\n",
    "\n",
    "# Hacemos el fit del modelo \n",
    "clf.fit(\n",
    "    X_train=X_train, y_train=y_train, # datos de entrenamiento\n",
    "    eval_set=[(X_val, y_val)], # datos de validación para evaluar el modelo durante el entrenamiento\n",
    "    eval_name=['val'], # nombre del conjunto de evaluación        \n",
    "    eval_metric=['auc'], # como métrica de evaluación escogemos AUC (Area Under the Curve) porque es adecuada para problemas de clasificación binaria con clases desbalanceadas\n",
    "    max_epochs=100, # número máximo de epochs para entrenar el modelo          \n",
    "    patience=10, # si la métrica de evaluación no mejora durante 10 epochs, se detiene el entrenamiento temprano\n",
    "    batch_size=1024, # tamaño del batch para el entrenamiento\n",
    "    virtual_batch_size=128, # tamaño del batch virtual para la normalización por lotes\n",
    "    num_workers=4, # número de trabajadores para cargar los datos y mejorar la paralelización\n",
    "    drop_last=False, # si el último batch es más pequeño que el tamaño del batch, no se descarta\n",
    "    weights=1 # no aplicamos pesos a las clases en este caso ya que TabNet maneja bien el desbalanceo\n",
    ")\n",
    "\n",
    "# Hacemos el predict y también obtenemos las probabilidades para el conjunto de validación\n",
    "probs = clf.predict_proba(X_test_final)[:, 1]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325d88cf",
   "metadata": {},
   "source": [
    "#### PARTE 5: EVALUACIÓN DE RESULTADOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9fb4b277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report (Threshold > 0.3):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.95      0.97    553574\n",
      "           1       0.07      0.95      0.13      2145\n",
      "\n",
      "    accuracy                           0.95    555719\n",
      "   macro avg       0.53      0.95      0.55    555719\n",
      "weighted avg       1.00      0.95      0.97    555719\n",
      "\n",
      "Verdaderos positivos (TP, Fraudes detectados): 2031\n",
      "Falsos positivos (FP, Transacciones legítimas detectadas como fraude): 27129\n",
      "Falsos Negativos (FN, Fraudes no detectados): 114\n",
      "Casos de fraudes totales: 2145\n",
      "Recall: 0.9469\n",
      "Ratio de falsos positivos (FPR): 0.0490\n"
     ]
    }
   ],
   "source": [
    "# En esta parte vamos a valorar los resultados obtenidos\n",
    "\n",
    "# Ajuste del umbral para optimizar el recall utilizando las probabilidades obtenidas\n",
    "threshhold = 0.3 # Ajustamos este threshhold según sea necesario, en este caso lo dejamos en 0.5 lo que significa que cualquier probabilidad por encima de 0.5 se clasifica como positiva (1).\n",
    "# Si reducimos el umbral, aumentaríamos el recall pero podríamos disminuir la precisión, ya que más casos serían clasificados como positivos, es decir, más falsos positivos.\n",
    "preds_recall_optimized = (probs >= threshhold).astype(int) # Convertimos las probabilidades a etiquetas binarias usando el umbral definido\n",
    "\n",
    "# Calculamos la matriz de confusión para obtener TP, FP, TN, FN\n",
    "cm = confusion_matrix(y_test_final, preds_recall_optimized)\n",
    "TN = cm[0, 0]\n",
    "FP = cm[0, 1]\n",
    "FN = cm[1, 0]\n",
    "TP = cm[1, 1]\n",
    "fraudes_totales = FN + TP\n",
    "\n",
    "# Evaluamos el modelo con el umbral optimizado y hacemos un print para ver los resultados\n",
    "print(f\"Classification Report (Threshold > {threshhold}):\")\n",
    "print(classification_report(y_test_final, preds_recall_optimized)) # Reporte de clasificación que incluye precisión, recall y F1-score\n",
    "\n",
    "print(f\"Verdaderos positivos (TP, Fraudes detectados): {TP}\")\n",
    "print(f\"Falsos positivos (FP, Transacciones legítimas detectadas como fraude): {FP}\")\n",
    "print(f\"Falsos Negativos (FN, Fraudes no detectados): {FN}\")\n",
    "print(f\"Casos de fraudes totales: {fraudes_totales}\")\n",
    "print(f\"Recall: {TP / fraudes_totales:.4f}\")\n",
    "print(f\"Ratio de falsos positivos (FPR): {FP / (FP + TN):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7ff5932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved model at tabnet_model.zip\n",
      "Arxius generats correctament: tabnet_model.zip, scaler.pkl, model_columns.pkl\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# 1. Guardar el model TabNet\n",
    "# Això crearà un fitxer 'tabnet_model.zip'\n",
    "clf.save_model('tabnet_model')\n",
    "\n",
    "# 2. Guardar el Scaler (RobustScaler)\n",
    "# És vital per escalar les dades noves igual que les d'entrenament\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "\n",
    "# 3. Guardar les COLUMNES d'entrenament (CRÍTIC)\n",
    "# Necessitem saber quines columnes exactes van entrar al model després del get_dummies\n",
    "# per assegurar que l'app web tingui la mateixa estructura.\n",
    "model_columns = list(train_data.drop(columns='is_fraud').columns)\n",
    "joblib.dump(model_columns, 'model_columns.pkl')\n",
    "\n",
    "print(\"Arxius generats correctament: tabnet_model.zip, scaler.pkl, model_columns.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
