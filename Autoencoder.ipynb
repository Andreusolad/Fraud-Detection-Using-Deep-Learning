{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d607660",
   "metadata": {},
   "source": [
    "## IMPORT DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46447640",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01236b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos los datos de entrenamiento y test desde archivos CSV. Como los datasets son muy grandes y tardan en cargarse, utilizamos una librería para paralelizar el código de carga.\n",
    "# Librería utilizada: multiprocessing\n",
    "from multiprocessing.pool import ThreadPool\n",
    "\n",
    "# Función para cargar los archivos csv que tenemos como datasets.\n",
    "def carregar(nom):\n",
    "    return pd.read_csv(nom)\n",
    "\n",
    "# Los csv que cargaremos son los siguientes:\n",
    "csvs = ['fraudTrain.csv', 'fraudTest.csv']\n",
    "\n",
    "# Creamos 2 grupos porque tenemoso dos archivos csv.\n",
    "pool = ThreadPool(2)\n",
    "\n",
    "# Cargamos los archivos en paralelo.\n",
    "resultats = pool.map(carregar, csvs)\n",
    "\n",
    "# Guardamos los resultados en sus respectivas variables de dataset de entrenamiento y test.\n",
    "train_data = resultats[0]\n",
    "test_data = resultats[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c88f887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1296675, 23)\n",
      "(555719, 23)\n"
     ]
    }
   ],
   "source": [
    "print(f'{train_data.shape}')\n",
    "print(f'{test_data.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b34066",
   "metadata": {},
   "source": [
    "### Transform data to timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45eda988",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.drop(columns='Unnamed: 0')\n",
    "test_data = test_data.drop(columns='Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55be86ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ahora, convertimos las fechas a números, específicamente a timestamps en segundos desde la época Unix (1 de enero de 1970).\n",
    "train_data['trans_date_trans_time'] = pd.to_datetime(train_data['trans_date_trans_time']).astype('int64') // 10**9\n",
    "test_data['trans_date_trans_time'] = pd.to_datetime(test_data['trans_date_trans_time']).astype('int64') // 10**9\n",
    "\n",
    "# Convertimos la columna 'dob' (fecha de nacimiento) a timestamps en segundos.\n",
    "train_data['dob'] = pd.to_datetime(train_data['dob']).astype('int64') // 10**9\n",
    "test_data['dob'] = pd.to_datetime(test_data['dob']).astype('int64') // 10**9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "472ebdfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trans_date_trans_time</th>\n",
       "      <th>cc_num</th>\n",
       "      <th>merchant</th>\n",
       "      <th>category</th>\n",
       "      <th>amt</th>\n",
       "      <th>first</th>\n",
       "      <th>last</th>\n",
       "      <th>gender</th>\n",
       "      <th>street</th>\n",
       "      <th>city</th>\n",
       "      <th>...</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>city_pop</th>\n",
       "      <th>job</th>\n",
       "      <th>dob</th>\n",
       "      <th>trans_num</th>\n",
       "      <th>unix_time</th>\n",
       "      <th>merch_lat</th>\n",
       "      <th>merch_long</th>\n",
       "      <th>is_fraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1592741665</td>\n",
       "      <td>2291163933867244</td>\n",
       "      <td>fraud_Kirlin and Sons</td>\n",
       "      <td>personal_care</td>\n",
       "      <td>2.86</td>\n",
       "      <td>Jeff</td>\n",
       "      <td>Elliott</td>\n",
       "      <td>M</td>\n",
       "      <td>351 Darlene Green</td>\n",
       "      <td>Columbia</td>\n",
       "      <td>...</td>\n",
       "      <td>33.9659</td>\n",
       "      <td>-80.9355</td>\n",
       "      <td>333497</td>\n",
       "      <td>Mechanical engineer</td>\n",
       "      <td>-56419200</td>\n",
       "      <td>2da90c7d74bd46a0caf3777415b3ebd3</td>\n",
       "      <td>1371816865</td>\n",
       "      <td>33.986391</td>\n",
       "      <td>-81.200714</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   trans_date_trans_time            cc_num               merchant  \\\n",
       "0             1592741665  2291163933867244  fraud_Kirlin and Sons   \n",
       "\n",
       "        category   amt first     last gender             street      city  \\\n",
       "0  personal_care  2.86  Jeff  Elliott      M  351 Darlene Green  Columbia   \n",
       "\n",
       "   ...      lat     long  city_pop                  job       dob  \\\n",
       "0  ...  33.9659 -80.9355    333497  Mechanical engineer -56419200   \n",
       "\n",
       "                          trans_num   unix_time  merch_lat  merch_long  \\\n",
       "0  2da90c7d74bd46a0caf3777415b3ebd3  1371816865  33.986391  -81.200714   \n",
       "\n",
       "   is_fraud  \n",
       "0         0  \n",
       "\n",
       "[1 rows x 22 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3d14d6",
   "metadata": {},
   "source": [
    "#### CODIFICAMOS LOS DATOS CON UN MODELO HÍBRIDO (ONE-HOT ENCODER Y ORDINAL ENCODER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7aa3f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "\n",
    "# Eliminamos las columnas de IDs que no aportan valor a la predicción y pueden causar sobreajuste. Tanto como para los datos de entrenamiento como de test.\n",
    "train_data = train_data.drop(columns=['cc_num', 'trans_num'])\n",
    "test_data = test_data.drop(columns=['cc_num', 'trans_num'])\n",
    "\n",
    "# Columnas buenas candidatas para OneHotEncoder (pocos valores únicos):\n",
    "low_cardinality_cols = ['gender', 'category'] \n",
    "\n",
    "# Columnas buenas candidatas para OrdinalEncoder (muchos valores únicos):\n",
    "high_cardinality_cols = ['merchant', 'job', 'city', 'state', 'street', 'first', 'last']\n",
    "\n",
    "# Aquí aplicamos OneHotEncoder a las columnas ya definidas.\n",
    "encoder_onehot = OneHotEncoder(handle_unknown='ignore', sparse_output=False, dtype=int) # Con el handle_unknown='ignore' evitamos errores si en test hay categorías no vistas en train.\n",
    "\n",
    "# Ahora aplicamos OrdinalEncoder a las columnas ya definidas.\n",
    "encoder_ordinal = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1, dtype=int) # handle_unknown='use_encoded_value' con unknown_value=-1 asigna -1 a categorías no vistas en test.\n",
    "\n",
    "# Entrenamos los codificadores con los datos de entrenamiento (fit).\n",
    "encoder_onehot.fit(train_data[low_cardinality_cols])\n",
    "encoder_ordinal.fit(train_data[high_cardinality_cols])\n",
    "\n",
    "# Función para aplicar las transformaciones en paralelo.\n",
    "def aplicar_encoders(df):\n",
    "    # Transformamos los datos (ajustados previamente con el train).\n",
    "    onehot_out = encoder_onehot.transform(df[low_cardinality_cols])\n",
    "    ordinal_out = encoder_ordinal.transform(df[high_cardinality_cols])\n",
    "    \n",
    "    # Convertimos los arrays resultantes a DataFrames para facilitar su manejo (para concatenar, etc.).\n",
    "    onehot_df = pd.DataFrame(onehot_out, index=df.index)\n",
    "    ordinal_df = pd.DataFrame(ordinal_out, index=df.index)\n",
    "    \n",
    "    # Eliminamos las columnas categóricas originales de ambos conjuntos de datos para luego añadir las codificadas.\n",
    "    df_temp = df.drop(columns=low_cardinality_cols + high_cardinality_cols)\n",
    "    \n",
    "    # Ahora concatenamos las nuevas columnas codificadas a los DataFrames originales. Ya sin las columnas categóricas originales.\n",
    "    return pd.concat([df_temp, onehot_df, ordinal_df], axis=1)\n",
    "\n",
    "# Creamos un grupo de 2 procesos para procesar train y test simultáneamente.\n",
    "pool_enc = ThreadPool(2)\n",
    "resultats_enc = pool_enc.map(aplicar_encoders, [train_data, test_data])\n",
    "\n",
    "# Guardamos los resultados en las correspondientes variables de train y test.\n",
    "train_data = resultats_enc[0]\n",
    "test_data = resultats_enc[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c268e69",
   "metadata": {},
   "source": [
    "## Entrenamos el Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ada7ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Època [1/100]\n",
      "Època [2/100]\n",
      "Època [3/100]\n",
      "Època [4/100]\n",
      "Època [5/100]\n",
      "Època [6/100]\n",
      "Època [7/100]\n",
      "Època [8/100]\n",
      "Època [9/100]\n",
      "Època [10/100]\n",
      "Època [11/100]\n",
      "Època [12/100]\n",
      "Època [13/100]\n",
      "Època [14/100]\n",
      "Època [15/100]\n",
      "Època [16/100]\n",
      "Època [17/100]\n",
      "Època [18/100]\n",
      "Època [19/100]\n",
      "Època [20/100]\n",
      "Època [21/100]\n",
      "Època [22/100]\n",
      "Època [23/100]\n",
      "Època [24/100]\n",
      "Època [25/100]\n",
      "Època [26/100]\n",
      "Època [27/100]\n",
      "Època [28/100]\n",
      "Època [29/100]\n",
      "Època [30/100]\n",
      "Època [31/100]\n",
      "Època [32/100]\n",
      "Època [33/100]\n",
      "Època [34/100]\n",
      "Època [35/100]\n",
      "Època [36/100]\n",
      "Època [37/100]\n",
      "Època [38/100]\n",
      "Època [39/100]\n",
      "Època [40/100]\n",
      "Època [41/100]\n",
      "Època [42/100]\n",
      "Època [43/100]\n",
      "Època [44/100]\n",
      "Època [45/100]\n",
      "Època [46/100]\n",
      "Època [47/100]\n",
      "Època [48/100]\n",
      "Època [49/100]\n",
      "Època [50/100]\n",
      "Època [51/100]\n",
      "Època [52/100]\n",
      "Època [53/100]\n",
      "Època [54/100]\n",
      "Època [55/100]\n",
      "Època [56/100]\n",
      "Època [57/100]\n",
      "Època [58/100]\n",
      "Època [59/100]\n",
      "Època [60/100]\n",
      "Època [61/100]\n",
      "Època [62/100]\n",
      "Època [63/100]\n",
      "Època [64/100]\n",
      "Època [65/100]\n",
      "Època [66/100]\n",
      "Època [67/100]\n",
      "Època [68/100]\n",
      "Època [69/100]\n",
      "Època [70/100]\n",
      "Època [71/100]\n",
      "Època [72/100]\n",
      "Època [73/100]\n",
      "Època [74/100]\n",
      "Època [75/100]\n",
      "Època [76/100]\n",
      "Època [77/100]\n",
      "Època [78/100]\n",
      "Època [79/100]\n",
      "Època [80/100]\n",
      "Època [81/100]\n",
      "Època [82/100]\n",
      "Època [83/100]\n",
      "Època [84/100]\n",
      "Època [85/100]\n",
      "Època [86/100]\n",
      "Època [87/100]\n",
      "Època [88/100]\n",
      "Època [89/100]\n",
      "Època [90/100]\n",
      "Època [91/100]\n",
      "Època [92/100]\n",
      "Època [93/100]\n",
      "Època [94/100]\n",
      "Època [95/100]\n",
      "Època [96/100]\n",
      "Època [97/100]\n",
      "Època [98/100]\n",
      "Època [99/100]\n",
      "Època [100/100]\n",
      "\n",
      "Millor loss aconseguit: 0.111585\n",
      "DIFERENTS THRESHOLD\n",
      "P95             (T=1.2479): Recall=0.046, Prec=0.0036, F1=0.0066, TP=99, FP=27679\n",
      "P99             (T=15.6438): Recall=0.005, Prec=0.0020, F1=0.0029, TP=11, FP=5536\n",
      "P90             (T=0.1904): Recall=0.543, Prec=0.0206, F1=0.0397, TP=1164, FP=55358\n",
      "MILLOR ESTRATÈGIA: P90\n",
      "Threshold: 0.190353\n",
      "\n",
      "MATRIU DE CONFUSIÓ\n",
      "\n",
      "True Negatives (TN):  498216 - Normals correctament classificades\n",
      "False Positives (FP):  55358 - Normals marcades com a frau\n",
      "False Negatives (FN):    981 - Fraus no detectats\n",
      "True Positives (TP):    1164 - Fraus detectats correctament ✓\n",
      "\n",
      "TAULA COMPARATIVA D'ESTRATÈGIES\n",
      "Strategy  Threshold   TP    FP   FN     TN   Recall  Precision       F1  F1_Weighted\n",
      "     P95   1.247854   99 27679 2046 525895 0.046154   0.003564 0.006617     0.006315\n",
      "     P99  15.643792   11  5536 2134 548038 0.005128   0.001983 0.002860     0.002412\n",
      "     P90   0.190353 1164 55358  981 498216 0.542657   0.020594 0.039682     0.038737\n"
     ]
    }
   ],
   "source": [
    "# Importamos las librerías de PyTorch, sklearn y matplotlib necesarias para construir, entrenar y evaluar el autoencoder.\n",
    "# Las librerías en concreto son, explicadas brevemente:\n",
    "# - torch: La librería principal de PyTorch para construir nuestros modelos de ML.\n",
    "# - torch.nn: Módulo de PyTorch que contiene clases y funciones para construir redes neuronales.\n",
    "# - torch.optim: Propociona algoritmos de optimización para entrenar modelos.\n",
    "# - DataLoader y TensorDataset: Para manejar conjuntos de datos y cargarlos en mini-batches.\n",
    "# - RobustScaler: Escalador de scikit-leaern muy útil para datos con outliers.\n",
    "# - confusion_matrix, classification_report, recall_score, precision_score, f1_score, roc_auc_score: Métricas de evaluación para visualizar los resultados.\n",
    "# - matplotlib.pyplot: Librería para crear gráficos.    \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report, recall_score, precision_score, f1_score, roc_auc_score\n",
    "\n",
    "\n",
    "# Separamos las features y target:\n",
    "X_train_np = train_data.drop(columns='is_fraud').values # values transforma de un pandas DataFrame a un numpy array.\n",
    "y_train_np = train_data['is_fraud'].values\n",
    "X_test_np = test_data.drop(columns='is_fraud').values\n",
    "y_test_np = test_data['is_fraud'].values\n",
    "\n",
    "\n",
    "# Cogemos solo las transacciones no fraudulentas para entrenar el autoencoder, que se encargará de aprender el patrón de las transacciones normales y luego detectar anomalías, que serían las fraudulentas.\n",
    "train_index_normal = np.where(y_train_np == 0)[0] \n",
    "X_train_normal = X_train_np[train_index_normal] # X_train_normal contiene solo las transacciones no fraudulentas.\n",
    "# Separamos los datos normales (no fraudulentos) para el entrenamiento, pero para el test usamos todos ellos, para las predicciones y que el modelo detecte anomalías como fraudes.\n",
    "\n",
    "\n",
    "# RobustScaler utiliza la mediana y el IQR, mejor para datos con outliers extremos (escalamos por columnas, no por filas y por tanto no afecta datos binarios como las one-hot encoded).\n",
    "# Como el dataset es muy grande, con millones de filas, vamos a paralelizar también el escalado porque suele ser un proceso lento y costoso.\n",
    "scaler = RobustScaler()\n",
    "scaler.fit(X_train_normal)\n",
    "\n",
    "# Función para aplicar la transformación del escalador en paralelo.\n",
    "def aplicar_escalado(datos):\n",
    "    return scaler.transform(datos)\n",
    "\n",
    "# Creamos un grupo de 2 hilos para transformar los datos de entrenamiento y test simultáneamente.\n",
    "pool_scaler = ThreadPool(2)\n",
    "resultados_escalado = pool_scaler.map(aplicar_escalado, [X_train_normal, X_test_np])\n",
    "\n",
    "# X_train_scaled contiene los datos normales escalados.\n",
    "X_train_scaled = resultados_escalado[0]\n",
    "# X_test_scaled contiene todos los datos de test escalados.\n",
    "X_test_scaled = resultados_escalado[1]\n",
    "\n",
    "#CONVERSIÓN A PYTORCH:\n",
    "train_tensor = torch.FloatTensor(X_train_scaled) \n",
    "test_tensor = torch.FloatTensor(X_test_scaled)\n",
    "\n",
    "batch_size = 256  # Más pequeño para mejor convergencia y menos memoria. Con batchs grandes puede que el modelo no aprenda bien, y con más pequeños el gradiente es más grande y permite mejores actualizaciones.\n",
    "# Train loader lo que hace es crear mini-batches de datos para entrenar el modelo de forma más eficiente. Es específico de PyTorch.\n",
    "# Para paralelizar esta parte del código, utilizamos num_workers=4 para cargar los datos en paralelo. Creamos 4 procesos que trabajan en paralelo.\n",
    "train_loader = DataLoader(TensorDataset(train_tensor), batch_size=batch_size, shuffle=True, num_workers=4) # quitamos el num_workers para que se ejecute en secuencial.\n",
    "\n",
    "\n",
    "\n",
    "# Ahora definimos el autoencoder con PyTorch. Esta clase define la arquitectura del modelo con las capas del encoder y decoder.\n",
    "\n",
    "class DeepFraudAutoencoder(nn.Module): # Heredamos de nn.Module, la clase base para todos los modelos de PyTorch.\n",
    "    \n",
    "    # Arquitectura del autoencoder con un encoder y decoder más profundos y con dropout para evitar overfitting.\n",
    "    # Aquí el modelo lo que hace es descomponer los datos de entrada en una representación comprimida (encoder) y luego reconstruirlos (decoder). Luego, al comparar la entrada original\n",
    "    # con la reconstruida, podemos detectar anomalías (fraudes) basándonos en el error de reconstrucción, ya que las transacciones fraudulentas deberían tener un error mayor al no seguir \n",
    "    # el patrón aprendido de las transacciones normales y presentar mayores anomalías.\n",
    "    \n",
    "    def __init__(self, input_size):\n",
    "        super(DeepFraudAutoencoder, self).__init__() # Super llama al constructor de la clase base nn.Module.\n",
    "        \n",
    "        # Encoder más profundo con dropout y ReLU como función de activación, que ayuda a capturar patrones complejos en los datos.\n",
    "        # Pasamos del número de columnas de entrada a una representación comprimida de 64, 48, 32, 16 y finalmente 8, el bottleneck.\n",
    "        # El dropout ayuda a prevenir el sobreajuste durante el entrenamiento. Lo que hace es desactivar aleatoriamente un porcentaje de neuronas en cada capa durante el entrenamiento, para \n",
    "        # evitar que el modelo dependa demasiado de ciertas neuronas y así mejorar su capacidad de aprendizaje general.\n",
    "        # El linear lo que hace es aplicar una transformación lineal a los datos de entrada, es decir, multiplica los datos por una matriz de pesos y añade un sesgo. Esto permite \n",
    "        # al modelo aprender relaciones lineales entre las características de entrada.\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            \n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            \n",
    "            nn.Linear(64, 48),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            \n",
    "            nn.Linear(48, 40),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(40, 32)  # Cuello de botella (bottleneck), es el final del encoder. Aquí los datos están más comprimidos y se supone que el modelo ha aprendido las características más importantes.\n",
    "        )\n",
    "        \n",
    "        # Decoder simètrico al encoder, pero en sentido contrario, para reconstruir los datos originales a partir de la representación comprimida. Es decir, a partir de un cuello de botella \n",
    "        # de 8 dimensiones, vamos expandiendo de nuevo a 16, 32, 48, 64 y finalmente al número de columnas originales. En esta reconstrución, el modelo intentará reconstruir los datos\n",
    "        # de entrada lo mejor posible. No obstante, cometerá errores, sobretodo en las transacciones fraudulentas porque esas no las ha visto durante el entrenamiento. Luego, al calcular el \n",
    "        # error cometido en la reconstrucción, teóricamente los mayores errores se daran en fraudes.\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(32, 40),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(40, 48),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            \n",
    "            nn.Linear(48, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            \n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            \n",
    "            nn.Linear(128, input_size)\n",
    "        )\n",
    "\n",
    "    # Definimos la función forward, que define cómo los datos fluyen a través del modelo. Esto lo que hace es pasar los datos de entrada por el encoder y luego por el decoder para obtener la reconstrucción.\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "\n",
    "# Ahora creamos una instancia del modelo. Definimos la dimensión de entrada que deberá ser exactamente igual al número de columnas de los datos de entrada, las features.\n",
    "input_dim = X_train_scaled.shape[1]\n",
    "model = DeepFraudAutoencoder(input_dim)\n",
    "\n",
    "# El criterio de pérdida que utilizaremos será el MSE (Mean Squared Error), que mide la diferencia cuadrática media entre los valores originales y los reconstruidos.\n",
    "# El optimizador utilizado es el AdamW, una variante de Adam que incluye decaimiento de peso, útil para evitar overfitting. \n",
    "# También definimos un scheduler que reduce la tasa de aprendizaje si la pérdida no mejora durante varias épocas.\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.5)\n",
    "\n",
    "# Luego, definimos el bucle de entrenamiento: especificamos el número de épocas, inicializamos variables para el early stopping y almacenamos las pérdidas de entrenamiento.\n",
    "EPOCHS = 100\n",
    "best_loss = float('inf') # Inicializamos la mejor pérdida con infinito, para que cualquier pérdida calculada sea mejor al principio.\n",
    "notepoch = 15 # Con esto, decimos que pare el entrenamiento si no mejora la pérdida en 15 épocas consecutivas.\n",
    "notepoch_counter = 0 # Esto es el contador de épocas sin mejora\n",
    "train_losses = [] # Guardamos los errores de entrenamiento.\n",
    "best_model_weights = None  # Guardar el mejor modelo en memoria\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for data in train_loader:\n",
    "        inputs = data[0]\n",
    "        \n",
    "        # Forward para obtener las salidas y calcular la pérdida. Esta parte lo que hace es pasar los datos de entrada por el modelo para obtener las reconstrucciones y \n",
    "        # luego calcular la pérdida entre las entradas originales y las reconstruidas.\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, inputs)\n",
    "        \n",
    "        # Backward para actualizar los pesos. Esta parte lo que hace es calcular los gradientes de la pérdida con respecto a los pesos del modelo y luego actualizar \n",
    "        # esos pesos utilizando el optimizador.\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Clipping de los gradientes para evitar que se vuelvan demasiado grandes, lo que puede causar inestabilidad en el entrenamiento. Esto se trata de \n",
    "        # limitar la norma de los gradientes a un valor máximo. La norma máxima impuesta es 1.0.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step() # Este paso actualiza los pesos del modelo basándose en los gradientes calculados.\n",
    "        \n",
    "        train_loss += loss.item() # Acumulamos la pérdida de entrenamiento de este batch, para luego calcular la pérdida media al final de la época.\n",
    "    \n",
    "    # Una vez hemos pasado por todos los batches, calculamos la pérdida media de la época y la almacenamos.\n",
    "    avg_loss = train_loss / len(train_loader)\n",
    "    train_losses.append(avg_loss) # Lo guardamos para diferentes épocas.\n",
    "    \n",
    "    # Para ver la época cuando ejecutamos el código.\n",
    "    print(f\"Època [{epoch+1}/{EPOCHS}]\")    \n",
    "    \n",
    "    \n",
    "    # Para controlar el early stopping, comprobamos si la pérdida ha mejorado. Si no mejora durante ciertas épocas consecutivas, paramos el entrenamiento.\n",
    "    \n",
    "    if avg_loss < best_loss:\n",
    "        # Aquí, si el error es menor que la best_loss, significa que hemos mejorado. Entonces, best_loss pasa a ser el avg_loss actual, y reseteamos el contador de épocas sin mejora.\n",
    "        best_loss = avg_loss\n",
    "        notepoch_counter = 0\n",
    "        best_model_weights = model.state_dict().copy()  # Guardar pesos trobats\n",
    "    else:\n",
    "        # Aquí, si no hemos mejorado, incrementamos el contador de épocas.\n",
    "        notepoch_counter += 1\n",
    "        if notepoch_counter >= notepoch:\n",
    "            # Si llegamos al límite de número de épocas máximo sin mejora, aturamos el entrenamiento.\n",
    "            print(f\"Early stopping a l'època {epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    scheduler.step(avg_loss) # Actualizamos el scheduler con la pérdida media de la época. Esto ajustará la tasa de aprendizaje si es necesario.\n",
    "    \n",
    "\n",
    "\n",
    "# Cogemos los pesos del mejor modelo guardados durante el entrenamiento y los cargamos en el modelo actual.\n",
    "model.load_state_dict(best_model_weights)\n",
    "print(f\"\\nMillor loss aconseguit: {best_loss:.6f}\")\n",
    "\n",
    "\n",
    "# Ahora, evaluamos el modelo en los datos de test para detectar fraudes. Calculamos las reconstrucciones y luego el error de reconstrucción para cada transacción. Después,\n",
    "# miramos si el error supera un threshold impuesto y, si lo supera, la transacción se clasifica como fraude, si no como normal.\n",
    "\n",
    "# Evaluación del modelo y cálculo de errores:\n",
    "model.eval() \n",
    "with torch.no_grad():\n",
    "    # Ahora hacemos las reconstrucciones de los datos de test. El input son todas las transacciones de test, tanto normales como fraudulentas, y el output son las reconstrucciones.\n",
    "    reconstructions = model(test_tensor)\n",
    "    # Calculalmos los errores de reconstrucción entre los datos originales y las reconstrucciones.\n",
    "    mse_loss = torch.mean((test_tensor - reconstructions) ** 2, dim=1).cpu().numpy()\n",
    "\n",
    "# Guardamos el error en un diccionario, por si quisieramos también calcular más errores come el MAE, etc.\n",
    "error_df = pd.DataFrame({\n",
    "    'mse': mse_loss,\n",
    "    'true_class': y_test_np\n",
    "})\n",
    "\n",
    "# Calculamos el normal_mse y fraud_mse para ver el error en cada clase.\n",
    "normal_mse = error_df[error_df['true_class'] == 0]['mse']\n",
    "fraud_mse = error_df[error_df['true_class'] == 1]['mse']\n",
    "\n",
    "\n",
    "print(\"DIFERENTS THRESHOLD\")\n",
    "\n",
    "# Probamos distintos valores de threshold basados en valores típicos como percentiles del error de las transacciones normales. Entonces, podemos ver que transacciones estan por encmia\n",
    "# de estos errores y marcarlos como fraudes.\n",
    "strategies = {\n",
    "    'P95': np.percentile(normal_mse, 95), # Error bajo el cual se encuentran el 95% de las transacciones normales.\n",
    "    'P99': np.percentile(normal_mse, 99), # Error bajo el cual se encuentran el 99% de las transacciones normales.\n",
    "    'P90': np.percentile(normal_mse, 90), # Error bajo el cual se encuentran el 90% de las transacciones normales.\n",
    "}\n",
    "\n",
    "\n",
    "# Evaluamos cada estrategia de threshold y calculamos las métricas de evaluación. Muy importante fijarnos en los valores de los TP, FP y FN.\n",
    "results = []\n",
    "for name, threshold in strategies.items():\n",
    "    # Cogemos el error de las predicciones y lo comparamos con el threshold establecido para clasificar como fraude o transacción normal.\n",
    "    # Luego y_pred es una lista donde 1 representa fraude y 0 normal.\n",
    "    y_pred = (mse_loss > threshold).astype(int)\n",
    "    \n",
    "    # Calculamos la matriz de confusión y las métricas de recall, precision y f1-score.\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test_np, y_pred).ravel()\n",
    "    recall = recall_score(y_test_np, y_pred)\n",
    "    precision = precision_score(y_test_np, y_pred)\n",
    "    f1 = f1_score(y_test_np, y_pred)\n",
    "    \n",
    "    # Finalmente guardamos los resultados en un diccionario para poder acceder facilmente a ellos después y compararlos.\n",
    "    results.append({\n",
    "        'Strategy': name,\n",
    "        'Threshold': threshold,\n",
    "        'TP': tp,\n",
    "        'FP': fp,\n",
    "        'FN': fn,\n",
    "        'TN': tn,\n",
    "        'Recall': recall,\n",
    "        'Precision': precision,\n",
    "        'F1': f1\n",
    "    })\n",
    "    \n",
    "    print(f\"{name:15s} (T={threshold:.4f}): Recall={recall:.3f}, Prec={precision:.4f}, F1={f1:.4f}, TP={tp}, FP={fp}\")\n",
    "\n",
    "\n",
    "# Transformamos el diccionario de resultados a un DataFrame para facilitar el análisis y cálculo de métricas, tal y como veremos a continuación.\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Ahora, ponemos un peso mayor al recall en el cálculo del F1-score, ya que en detección de fraudes es más importante detectar la mayoría de fraudes posibles (recall alto),\n",
    "# aunque eso implique tener más falsos positivos (precision más baja). Por eso, damos un peso del 60% al recall y 40% a la precision.\n",
    "results_df['F1_Weighted'] = 2 * (0.6 * results_df['Recall'] * results_df['Precision']) / (0.6 * results_df['Recall'] + results_df['Precision'])\n",
    "# Para hallar el mejor resultado, buscamos cuál tiene el F1_Weighted mayor, ya que será el que mejor balancea recall y precision según las condiciones impuestas.\n",
    "best_result = results_df.loc[results_df['F1_Weighted'].idxmax()]\n",
    "\n",
    "print(f\"MILLOR ESTRATÈGIA: {best_result['Strategy']}\")\n",
    "print(f\"Threshold: {best_result['Threshold']:.6f}\")\n",
    "\n",
    "# Ahora, aplicamos el mejor threshold encontrado para hacer las predicciones finales.\n",
    "best_threshold = best_result['Threshold']\n",
    "y_pred_best = (mse_loss > best_threshold).astype(int)\n",
    "\n",
    "\n",
    "# Ahora vamos a mostrar la matriz de confusión del mejor modelo para ver cómo ha clasificado las transacciones.\n",
    "print(\"\\nMATRIU DE CONFUSIÓ\")\n",
    "cm = confusion_matrix(y_test_np, y_pred_best)\n",
    "# Más específicamente, en TN, FP, FN, TP:\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "print(f\"\\nTrue Negatives (TN):  {tn:6d} - Normals correctament classificades\")\n",
    "print(f\"False Positives (FP): {fp:6d} - Normals marcades com a frau\")\n",
    "print(f\"False Negatives (FN): {fn:6d} - Fraus no detectats\")\n",
    "print(f\"True Positives (TP):  {tp:6d} - Fraus detectats correctament ✓\")\n",
    "\n",
    "\n",
    "# Mostramos la tabla final de resultados:\n",
    "print(\"\\nTAULA COMPARATIVA D'ESTRATÈGIES\")\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5852efaa",
   "metadata": {},
   "source": [
    "TAULA COMPARATIVA D'ESTRATÈGIES (resultat amb el bottleneck comentat i mse)\n",
    "Strategy  Threshold   TP    FP   FN   Recall  Precision       F1  F1_Weighted\n",
    "     P95   2.330554   97 27679 2048 0.045221   0.003492 0.006484     0.006188\n",
    "     P99  26.827852   11  5536 2134 0.005128   0.001983 0.002860     0.002412\n",
    "     P90   0.350100 1121 55358 1024 0.522611   0.019848 0.038244     0.037333\n",
    "\n",
    "F1-Score:                  0.0382 - Mitjana harmònica\n",
    "Accuracy:                  0.8985\n",
    "False Positive Rate (FPR): 0.1000 - % normals marcades com frau\n",
    "AUC-ROC:                   0.7896"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
